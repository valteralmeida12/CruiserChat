# CruiserChat

This is a cpp AI inference chat bot, where the user can run any LLM locally and offline.

## Setup

To setup the project (run setup script):
```bash
./setup.sh
```

## Running the App

To run the app with the default model:
```bash
cd build
./CruiserChat
```

To run the app with a different model:
```bash
./CruiserChat <model-path>
```

> **Note:** This is still a project under progress. GPU compilation and acceleration will be supported with future patches.
